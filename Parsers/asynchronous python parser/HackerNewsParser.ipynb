{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0201ca6-6869-43ca-9409-cca59eeeef1e",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "- Узнать параметры каждой отдельной статьи на 10 страницах Hacker News\n",
    "    - [x] rank - номер статьи в списке\n",
    "    - [x] id статьи\n",
    "    - [x] url статьи\n",
    "    - [x] titile - загловок статьи \n",
    "    - [x] points - span class='scrore' в subline\n",
    "    - [x] количество комментов к статье\n",
    "    - [x] load_time - время загрузки статьи "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50020c3-eda8-4217-82f1-c67e58a5d205",
   "metadata": {},
   "source": [
    "# Зависимости\n",
    "- python==3.10.8\n",
    "- requests==2.28.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad256eb6-ccd3-4115-a9dd-bcb5f64a8ee8",
   "metadata": {},
   "source": [
    "## Установка виртуальной среды и ядра \n",
    "\n",
    "Не волнуйтесь, в конце ноутбука все будет удалено, главное открывайте консоль или терминалы **в папке с ноутбуком**\n",
    "### Windows\n",
    "\n",
    "Нужно в командной строке ввести следующее и нажать Enter. Установка около минуты.\n",
    "```powershell\n",
    "python –m venv parser_env && parser_env\\Scripts\\activate && pip install anyio==3.6.2 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asttokens==2.1.0 attrs==22.1.0 backcall==0.2.0 beautifulsoup4==4.11.1 bleach==5.0.1 certifi==2022.9.24 cffi==1.15.1 charset-normalizer==2.1.1 colorama==0.4.6 debugpy==1.6.3 decorator==5.1.1 defusedxml==0.7.1 entrypoints==0.4 executing==1.2.0 fastjsonschema==2.16.2 idna==3.4 ipykernel==6.17.1 ipython==8.6.0 ipython-genutils==0.2.0 jedi==0.18.2 Jinja2==3.1.2 jsonschema==4.17.1 jupyter-server==1.23.3 jupyter_client==7.4.7 jupyter_core==5.0.0 jupyterlab-pygments==0.2.2 MarkupSafe==2.1.1 matplotlib-inline==0.1.6 mistune==2.0.4 nbclassic==0.4.8 nbclient==0.7.0 nbconvert==7.2.5 nbformat==5.7.0 nest-asyncio==1.5.6 notebook_shim==0.2.2 packaging==21.3 pandocfilters==1.5.0 parso==0.8.3 pickleshare==0.7.5 platformdirs==2.5.4 prometheus-client==0.15.0 prompt-toolkit==3.0.33 psutil==5.9.4 pure-eval==0.2.2 pycparser==2.21 Pygments==2.13.0 pyparsing==3.0.9 pyrsistent==0.19.2 python-dateutil==2.8.2 pywin32==305 pywinpty==2.0.9 pyzmq==24.0.1 requests==2.28.1 Send2Trash==1.8.0 six==1.16.0 sniffio==1.3.0 soupsieve==2.3.2.post1 stack-data==0.6.1 terminado==0.17.0 tinycss2==1.2.1 tornado==6.2 traitlets==5.5.0 urllib3==1.26.13 wcwidth==0.2.5 webencodings==0.5.1 websocket-client==1.4.2 && python -m ipykernel install --name=parser_kernel\n",
    "```\n",
    "\n",
    "### Linux\n",
    "\n",
    "Нужно в терминале ввести следующее и нажать Enter. Установка около минуты.\n",
    "```bash\n",
    "python –m venv parser_env && parser_env/Scripts/activate && pip install anyio==3.6.2 argon2-cffi==21.3.0 argon2-cffi-bindings==21.2.0 asttokens==2.1.0 attrs==22.1.0 backcall==0.2.0 beautifulsoup4==4.11.1 bleach==5.0.1 certifi==2022.9.24 cffi==1.15.1 charset-normalizer==2.1.1 colorama==0.4.6 debugpy==1.6.3 decorator==5.1.1 defusedxml==0.7.1 entrypoints==0.4 executing==1.2.0 fastjsonschema==2.16.2 idna==3.4 ipykernel==6.17.1 ipython==8.6.0 ipython-genutils==0.2.0 jedi==0.18.2 Jinja2==3.1.2 jsonschema==4.17.1 jupyter-server==1.23.3 jupyter_client==7.4.7 jupyter_core==5.0.0 jupyterlab-pygments==0.2.2 MarkupSafe==2.1.1 matplotlib-inline==0.1.6 mistune==2.0.4 nbclassic==0.4.8 nbclient==0.7.0 nbconvert==7.2.5 nbformat==5.7.0 nest-asyncio==1.5.6 notebook_shim==0.2.2 packaging==21.3 pandocfilters==1.5.0 parso==0.8.3 pickleshare==0.7.5 platformdirs==2.5.4 prometheus-client==0.15.0 prompt-toolkit==3.0.33 psutil==5.9.4 pure-eval==0.2.2 pycparser==2.21 Pygments==2.13.0 pyparsing==3.0.9 pyrsistent==0.19.2 python-dateutil==2.8.2 pywin32==305 pywinpty==2.0.9 pyzmq==24.0.1 requests==2.28.1 Send2Trash==1.8.0 six==1.16.0 sniffio==1.3.0 soupsieve==2.3.2.post1 stack-data==0.6.1 terminado==0.17.0 tinycss2==1.2.1 tornado==6.2 traitlets==5.5.0 urllib3==1.26.13 wcwidth==0.2.5 webencodings==0.5.1 websocket-client==1.4.2 python -m ipykernel install --name=parser_kernel\n",
    "```\n",
    "\n",
    "### Mac\n",
    "Если у вас мак, то устанавливайте виртуальную среду и ядро сами:)\n",
    "\n",
    "## Далее нужно перезагрузить jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d5c138f-e9f6-4195-bbb2-d78211e6def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exit() # restart kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9048f6-7e23-4b81-9360-23ff658e3990",
   "metadata": {},
   "source": [
    "## После этого нужно сменить ядро ipynb на parser_kernel\n",
    "![img](https://github.com/Sly-Dog/attachments/blob/main/parser_kernel.png?raw=true)\n",
    "Для того, чтобы оно появилось после рестарта, бывает нужно подождать секунд 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ac32fe-9b34-4994-ad5d-f1cbcd9436e1",
   "metadata": {},
   "source": [
    "# Импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09b4a03-e586-4716-a3c1-4ad047a958e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "import requests\n",
    "from time import sleep, perf_counter\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import asyncio\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad279279-0896-45ea-875d-367e77dc0af6",
   "metadata": {},
   "source": [
    "# Функции "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67742549-88e7-48e9-a5aa-688bec3a825a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''writes data to CSV file'''\n",
    "def write_to_csv(name, dicts):\n",
    "    field_names = dicts[0].keys()\n",
    "\n",
    "\n",
    "    with open(f'{name}.csv', 'w', encoding=\"utf-8\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=field_names)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42bc118-02b8-48d8-990f-09f0e14b40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for sending requests, geting load time of the pages'''\n",
    "class web_work():\n",
    "    def __init__(self):\n",
    "        \n",
    "        # this is needed for browser imitation\n",
    "        self.user_agent = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36\"\n",
    "        self.headers = {'User-Agent':self.user_agent}\n",
    "        \n",
    "    '''get HTML from Hacker News page'''    \n",
    "    def get_articles_page(self, p):\n",
    "        \n",
    "        self.p = p\n",
    "        sleep(random() * 0.6) # human click imitation\n",
    "        \n",
    "        response = requests.get(f'https://news.ycombinator.com/news?p={self.p}'\n",
    "                                , headers=self.headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        return soup\n",
    "   \n",
    "    '''Get load_time for article'''\n",
    "    def get_load_time(self, article_url):\n",
    "        indicator = 0 \n",
    "\n",
    "        while indicator != 3: # 3 tryes to get response from url\n",
    "            try:\n",
    "\n",
    "                # send request with article_url\n",
    "                response = requests.get(\n",
    "                    article_url,\n",
    "                    headers=self.headers,\n",
    "                    stream=True,\n",
    "                    timeout=3) # 3 seconds is to much\n",
    "\n",
    "                load_time = response.elapsed.total_seconds()\n",
    "                return(load_time)\n",
    "\n",
    "            except:\n",
    "                print('Can not load site:', article_url)\n",
    "                indicator +=1 \n",
    "            load_time = None\n",
    "            \n",
    "        return load_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f92797-924f-4ee7-a490-7818164b97e4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee82ab1-ecca-4b6c-b2b8-0d7fda4c52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Collect needed data from HTML coe'''\n",
    "class articles_info():  \n",
    "    \n",
    "    \n",
    "    def __init__(self, soup): \n",
    "    \n",
    "        self.tops = soup.select(\"tr.athing\")     # list of titles\n",
    "        self.bottoms = soup.select(\"td.subtext\") # list of sublines\n",
    "        \n",
    "        self.get_info = {} # here will be placed info about all articles\n",
    "    \n",
    "    '''Getting data from HTML soup'''\n",
    "    def get_article_data(self, article_num):\n",
    "        ### TITLE DATA\n",
    "        #######################################################################\n",
    "        top = self.tops[article_num] # get title                              #\n",
    "\n",
    "        article_id = int(top.get('id'))                                   # id \n",
    "        rank =int(top.find('span', class_='rank').string.replace('.','')) # rank\n",
    "        title = top.find('span', class_='titleline').a.string             # title\n",
    "\n",
    "      # url                                                       \n",
    "        raw_url = top.find('span', class_='titleline').a.get('href')\n",
    "        domain = 'https://news.ycombinator.com/'                    \n",
    "\n",
    "      # iternal link check                                         \n",
    "        if 'https://' not in raw_url:                               \n",
    "            url = domain+raw_url                                    \n",
    "        else: url = raw_url\n",
    "        \n",
    "        #######################################################################\n",
    "\n",
    "        ### SUBTEXT DATA\n",
    "        #######################################################################\n",
    "        bottom = self.bottoms[article_num] # get subline                      #\n",
    "      # comments    \n",
    "        try: comment_count = int(re.search('[0-9]+',                                   \n",
    "                re.search('[0-9]+\\scomme', str(bottom) ) .group()).group())   \n",
    "        except: comment_count = -1\n",
    "\n",
    "      # points                                                               \n",
    "        try: points = int(bottom.select(\"span.score\"\n",
    "                                       )[0].string.replace(' points','')) \n",
    "        except: points = -1                                                   #\n",
    "        #######################################################################\n",
    "        \n",
    "      # adding info about artickle to dict\n",
    "        self.get_info[article_num] = {\n",
    "                      'id' : article_id,\n",
    "                      'rank' : rank,\n",
    "                      'url' : url,\n",
    "                      'title' : title,\n",
    "                      'points' : points,\n",
    "                      'comment_count' : comment_count}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc7d08-01aa-4b3d-94a5-9c86b0ef0053",
   "metadata": {},
   "source": [
    "# Главный скрипт "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be2397f-0917-4602-8442-ee8fe6f80db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_art = web_work() # initialize the class\n",
    "\n",
    "pages_with_articles_info = {} # storage for all data\n",
    "\n",
    "### fill the pages_with_articles_info with data\n",
    "for p in range(10):\n",
    "    \n",
    "    soup = web_art.get_articles_page(p) # using get_article function\n",
    "    info_art = articles_info(soup)\n",
    "    \n",
    "    for article_n in range(len(info_art.tops)):\n",
    "        \n",
    "        info_art.get_article_data(article_n) # using get_article_data function \n",
    "        \n",
    "        \n",
    "    pages_with_articles_info[p] = info_art.get_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ce278f-eb5e-4e8f-8be9-254aaf0f7497",
   "metadata": {},
   "outputs": [],
   "source": [
    "### get load_time list asynchronously ###\n",
    "\n",
    "loop = asyncio.get_event_loop() # needed for asyncio in jupyter\n",
    "\n",
    "url_list = []\n",
    "for pages in pages_with_articles_info.values():\n",
    "    for article in pages.values():\n",
    "        url_list.append(article['url'])\n",
    "if len(url_list) != 300: raise Exception(\"НЕ 300. \\\n",
    "список url не равен количеству статей\")\n",
    "\n",
    "# using get_load_time function\n",
    "async def async_get_load_time(article_url: str): \n",
    "    return await asyncio.to_thread(web_art.get_load_time, article_url)\n",
    "\n",
    "async def main():\n",
    "    global a, b\n",
    "    \n",
    "    a = perf_counter()\n",
    "    result = await asyncio.gather(*[async_get_load_time(url) for url in url_list])\n",
    "    b = perf_counter()\n",
    "        \n",
    "    return result\n",
    "\n",
    "load_time_wait = loop.create_task(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9737da04-d01b-4f18-a024-60361b358625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can not load site: https://twitter.com/wolfiechristl/status/1596277060435345411\n",
      "Can not load site: https://twitter.com/wolfiechristl/status/1596277060435345411\n",
      "Can not load site: https://twitter.com/wolfiechristl/status/1596277060435345411\n",
      "Can not load site: https://twitter.com/wolfiechristl/status/1596277060435345411\n",
      "Can not load site: https://twitter.com/wolfiechristl/status/1596277060435345411\n",
      "Can not load site: https://twitter.com/wolfiechristl/status/1596277060435345411\n",
      "Can not load site: https://twitter.com/angealbertini/status/1595845629510647808\n",
      "Can not load site: https://twitter.com/angealbertini/status/1595845629510647808\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://twitter.com/angealbertini/status/1595845629510647808\n",
      "Can not load site: https://www.bbc.com/news/world-australia-63699884\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://www.bbc.com/news/world-australia-63699884\n",
      "Can not load site: https://twitter.com/emostaque/status/1596222094877945856\n",
      "Can not load site: https://twitter.com/emostaque/status/1596222094877945856\n",
      "Can not load site: https://twitter.com/emostaque/status/1596222094877945856\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://www.newyorker.com/magazine/2022/11/28/how-the-huxleys-electrified-evolution\n",
      "Can not load site: https://engineering.fb.com/2022/11/21/production-engineering/future-computing-ptp/\n",
      "Can not load site: https://engineering.fb.com/2022/11/21/production-engineering/future-computing-ptp/\n",
      "Can not load site: https://engineering.fb.com/2022/11/21/production-engineering/future-computing-ptp/\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://www.bbc.co.uk/news/world-us-canada-63764450\n",
      "Can not load site: https://www.bbc.co.uk/news/world-us-canada-63764450\n",
      "Can not load site: https://www.bbc.co.uk/news/world-us-canada-63764450\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://www.bbc.com/news/world-australia-63699884\n",
      "Can not load site: https://twitter.com/jagarikin/status/1593771091738374144\n",
      "Can not load site: https://twitter.com/jagarikin/status/1593771091738374144\n",
      "Can not load site: https://twitter.com/jagarikin/status/1593771091738374144\n",
      "Can not load site: https://twitter.com/brendaneich/status/1596039777060388864\n",
      "Can not load site: https://twitter.com/brendaneich/status/1596039777060388864\n",
      "Can not load site: https://twitter.com/brendaneich/status/1596039777060388864\n",
      "Can not load site: https://soundraw.io/\n",
      "Can not load site: https://uokik.gov.pl/aktualnosci.php?news_id=19070\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://uokik.gov.pl/aktualnosci.php?news_id=19070\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Can not load site: https://uokik.gov.pl/aktualnosci.php?news_id=19070\n",
      "Can not load site: https://www.bbc.com/news/technology-63743597\n",
      "Время работы скрипта: 22.44s, \n",
      "Длина получившегося списка: 300\n"
     ]
    }
   ],
   "source": [
    "### awaiting for load_time list ##\n",
    "\n",
    "load_time_list = await load_time_wait\n",
    "script_time = b - a\n",
    "print(f'''Время работы скрипта: {round(script_time, 2)}s, \n",
    "Длина получившегося списка: {len(load_time_list)}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5bee3-dac5-4abd-b5da-1418f6126800",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**Вариант без асинхронности будет загружаться 6 минут**\n",
    "\n",
    "```ipython\n",
    "%%time\n",
    "for i in pages_with_articles_info.values():\n",
    "    for j in i.values(): j['load_time'] = get_load_time(j['url'], headers)\n",
    "```\n",
    "\\>>>\n",
    "```\n",
    "CPU times: total: 6.19 s\n",
    "Wall time: 6min 17s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9072605e-fe27-433f-a0d4-2eaa9a82e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inserting load_time list to the other data ###\n",
    "counter = 0\n",
    "for i in pages_with_articles_info.values():\n",
    "    for j in i.values():\n",
    "        j['load_time'] = load_time_list[counter]\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58cd0dc8-096a-4fe0-925e-4a6a8371067f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write data in a file ###\n",
    "\n",
    "dicts = []\n",
    "\n",
    "for page_dict in pages_with_articles_info.values():\n",
    "    for article_dict in page_dict.values():\n",
    "        dicts.append(article_dict)\n",
    "        \n",
    "        \n",
    "write_to_csv('articles', dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9844e2a-cf55-4bb0-883d-76a471ac44d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Удаление ядра и виртуальной среды\n",
    "\n",
    "Нужно сменить ядро на любое другое, например на Python 3(ipykernel). Затем выполнить код в ячеке ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b49c6e91-54c8-4611-8aca-2544670d3e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[RemoveKernelSpec] Removed C:\\ProgramData\\jupyter\\kernels\\parser_kernel\n"
     ]
    }
   ],
   "source": [
    "# remove parser_kernel\n",
    "!jupyter kernelspec remove parser_kernel -y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535de9f3-3c31-45d0-b486-3ffc779893d7",
   "metadata": {},
   "source": [
    "Ядро удалено. Осталось удалить папку parser_env в директории этого ноутбука и готово."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parser_kernel",
   "language": "python",
   "name": "parser_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
